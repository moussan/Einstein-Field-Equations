version: '3.8'

services:
  frontend:
    image: einstein-frontend:${TAG}
    build:
      context: ./src/frontend
      args:
        - NODE_ENV=production
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      replicas: 3
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
    environment:
      - OTEL_COLLECTOR_URL=http://otel-collector:4318/v1/traces

  edge-functions:
    image: einstein-edge-functions:${TAG}
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  prometheus:
    image: pro./config/prometheus:v2.45.0
    volumes:
      - ./config/prometheus:/et./config/prometheus
      - prometheus_data./config/prometheus
    command:
      - '--config.file=/et./config/prometheus/prometheus.yml'
      - '--storage.tsdb.path./config/prometheus'
      - '--web.console.libraries=/usr/shar./config/prometheus/console_libraries'
      - '--web.console.templates=/usr/shar./config/prometheus/consoles'
    ports:
      - "9090:9090"

  grafana:
    image: grafana/grafana:10.0.0
    volumes:
      - grafana_data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    ports:
      - "3000:3000"

  alertmanager:
    image: prom/alertmanager:v0.25.0
    volumes:
      - ./alertmanager:/etc/alertmanager
    command:
      - '--config.file=/etc/alertmanager/config.yml'
      - '--storage.path=/alertmanager'
    ports:
      - "9093:9093"

  node-exporter:
    image: prom/node-exporter:v1.6.0
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    ports:
      - "9100:9100"

  fluentd:
    image: fluen./config/fluentd:v1.16-1
    volumes:
      - ./config/fluentd/conf./config/fluentd/etc
      - ./config/fluentd/certs./config/fluentd/certs
      - fluentd_data./config/fluentd/log
    ports:
      - "24224:24224"
      - "24224:24224/udp"

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.9.0
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"

  kibana:
    image: docker.elastic.co/kibana/kibana:8.9.0
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    ports:
      - "5601:5601"

  otel-collector:
    image: otel/opentelemetry-collector:0.81.0
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - "4317:4317"  # OTLP gRPC
      - "4318:4318"  # OTLP HTTP
      - "8889:8889"  # Prometheus exporter
      - "13133:13133" # Health check
      - "55679:55679" # ZPages

  jaeger:
    image: jaegertracing/all-in-one:1.47
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    ports:
      - "16686:16686" # UI
      - "14250:14250" # OTLP gRPC

  redis:
    image: redis:7.0-alpine
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"

  backup:
    image: einstein-backup:${TAG}
    volumes:
      - /backups:/backups
      - redis_data:/data
    environment:
      - DATABASE_URL=${DATABASE_URL}
    deploy:
      placement:
        constraints:
          - node.role == manager
    command: ["crond", "-f"]

volumes:
  prometheus_data:
  grafana_data:
  elasticsearch_data:
  fluentd_data:
  redis_data:

networks:
  app_network:
    driver: bridge
    name: einstein_network_prod
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16 
